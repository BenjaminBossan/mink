{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with linear networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mink's *layers* work exactly as sklearn *transformers*, and linear networks work analogously to sklearn Pipelines. Below, the usage of mink layers as transformers is illustrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mink import layers\n",
    "from mink import nonlinearities\n",
    "from mink import objectives\n",
    "from mink import updates\n",
    "from mink import make_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn toy classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_classes=5,\n",
    "    n_informative=10,\n",
    "    random_state=0,\n",
    ")\n",
    "y = LabelBinarizer().fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare a neural network by using sklearn's Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple linear network with an input layer, a hidden layer, and an output layer, by passing a list of name/layer tuples to an sklearn `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = Pipeline([\n",
    "    ('input', layers.InputLayer()),\n",
    "    ('dense-1', layers.DenseLayer()),\n",
    "    ('dense-2', layers.DenseLayer(\n",
    "        num_units=5,\n",
    "        nonlinearity=nonlinearities.Softmax(),\n",
    "        )),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altnernatively, we may use the `make_network` function from mink, which works analogously to sklearn's `make_pipeline`. This saves us the need to write a name for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = make_network([\n",
    "    layers.InputLayer(),\n",
    "    layers.DenseLayer(),\n",
    "    layers.DenseLayer(\n",
    "        num_units=5,\n",
    "        nonlinearity=nonlinearities.Softmax(),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('input', InputLayer(Xs=None, make_logs=False, name=None, ys=None)),\n",
      " ('dense-1',\n",
      "  DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
      "      incoming=InputLayer(Xs=None, make_logs=False, name=None, ys=None),\n",
      "      make_logs=False, name=None, nonlinearity=None, num_units=None)),\n",
      " ('dense-2',\n",
      "  DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
      "      incoming=DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
      "      incoming=InputLayer(Xs=None, make_logs=False, name=None, ys=None),\n",
      "      make_logs=False, name=None, nonlinearity=None, num_units=None),\n",
      "      make_logs=False, name=None, nonlinearity=Softmax(), num_units=5))]\n"
     ]
    }
   ],
   "source": [
    "pprint(network.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get symbolic output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the symbolic input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = tf.placeholder(dtype='float32', shape=(None, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = tf.placeholder(dtype='float32', shape=(None, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in sklearn, we can call `fit_transform` and the pipeline to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys_out = network.fit_transform(Xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = objectives.CrossEntropy()(ys, ys_out)\n",
    "train_step = updates.Momentum()(loss)\n",
    "inputs = [train_step, loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4433\n",
      "Loss: 0.9388\n",
      "Loss: 0.8456\n",
      "Loss: 0.7814\n",
      "Loss: 0.7275\n",
      "Loss: 0.6811\n",
      "Loss: 0.6404\n",
      "Loss: 0.6046\n",
      "Loss: 0.5724\n",
      "Loss: 0.5443\n",
      "Loss: 0.5191\n",
      "Loss: 0.4967\n",
      "Loss: 0.4761\n",
      "Loss: 0.4572\n",
      "Loss: 0.4401\n",
      "Loss: 0.4237\n",
      "Loss: 0.4097\n",
      "Loss: 0.3960\n",
      "Loss: 0.3839\n",
      "Loss: 0.3720\n",
      "Loss: 0.3611\n",
      "Loss: 0.3510\n",
      "Loss: 0.3416\n",
      "Loss: 0.3329\n",
      "Loss: 0.3247\n",
      "Loss: 0.3166\n",
      "Loss: 0.3095\n",
      "Loss: 0.3031\n",
      "Loss: 0.2959\n",
      "Loss: 0.2900\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "losses = []\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for epoch in range(30):\n",
    "        losses_epoch = []\n",
    "        for i in range((X.shape[0] + batch_size - 1) // batch_size):\n",
    "            Xb = X[i * batch_size:(i + 1) * batch_size]\n",
    "            yb = y[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            feed_dict = {Xs: Xb, ys: yb}\n",
    "            _, loss = session.run(inputs, feed_dict=feed_dict)\n",
    "            losses_epoch.append(loss)\n",
    "        losses_mean = np.mean(losses_epoch)\n",
    "        losses.append(losses_mean)\n",
    "        print(\"Loss: {:.4f}\".format(losses_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare a neural network by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can feedforward the output of each layer and pass the output to the next layer by hand. This will result in exactly the same network as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = layers.InputLayer(Xs, ys).fit_transform(Xs)\n",
    "out = layers.DenseLayer().fit_transform(out)\n",
    "out = layers.DenseLayer(num_units=5, nonlinearity=nonlinearities.Softmax()).fit_transform(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = objectives.CrossEntropy()(ys, out)\n",
    "train_step = updates.Momentum()(loss)\n",
    "inputs = [train_step, loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3519\n",
      "Loss: 0.9458\n",
      "Loss: 0.8517\n",
      "Loss: 0.7857\n",
      "Loss: 0.7328\n",
      "Loss: 0.6886\n",
      "Loss: 0.6500\n",
      "Loss: 0.6168\n",
      "Loss: 0.5873\n",
      "Loss: 0.5616\n",
      "Loss: 0.5370\n",
      "Loss: 0.5150\n",
      "Loss: 0.4949\n",
      "Loss: 0.4763\n",
      "Loss: 0.4591\n",
      "Loss: 0.4434\n",
      "Loss: 0.4292\n",
      "Loss: 0.4160\n",
      "Loss: 0.4036\n",
      "Loss: 0.3925\n",
      "Loss: 0.3821\n",
      "Loss: 0.3726\n",
      "Loss: 0.3635\n",
      "Loss: 0.3554\n",
      "Loss: 0.3469\n",
      "Loss: 0.3396\n",
      "Loss: 0.3316\n",
      "Loss: 0.3246\n",
      "Loss: 0.3177\n",
      "Loss: 0.3111\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "losses = []\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for epoch in range(30):\n",
    "        losses_epoch = []\n",
    "        for i in range((X.shape[0] + batch_size - 1) // batch_size):\n",
    "            Xb = X[i * batch_size:(i + 1) * batch_size]\n",
    "            yb = y[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            feed_dict = {Xs: Xb, ys: yb}\n",
    "            _, loss = session.run(inputs, feed_dict=feed_dict)\n",
    "            losses_epoch.append(loss)\n",
    "        losses_mean = np.mean(losses_epoch)\n",
    "        losses.append(losses_mean)\n",
    "        print(\"Loss: {:.4f}\".format(losses_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
