{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcasing recurrent neural networks in mink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This show case is taken from the [Lasagne examples](https://github.com/Lasagne/Lasagne/blob/master/examples/recurrent.py), all credit goes there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mink.layers import RecurrentLayer\n",
    "from mink.layers import LSTMLayer\n",
    "from mink.layers import GRULayer\n",
    "from mink.layers import InputLayer\n",
    "from mink.layers import DenseLayer\n",
    "from mink.nonlinearities import Tanh\n",
    "from mink.updates import RMSProp\n",
    "from mink import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0015\n",
    "MAX_EPOCHS = 40\n",
    "UPDATE = RMSProp(learning_rate=LEARNING_RATE)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "SESSION_KWARGS = {'config': tf.ConfigProto(gpu_options=gpu_options)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(min_length=50, max_length=55, n_batch=1000):\n",
    "    '''\n",
    "    Generate a batch of sequences for the \"add\" task, e.g. the target for the\n",
    "    following\n",
    "\n",
    "    ``| 0.5 | 0.7 | 0.3 | 0.1 | 0.2 | ... | 0.5 | 0.9 | ... | 0.8 | 0.2 |\n",
    "      |  0  |  0  |  1  |  0  |  0  |     |  0  |  1  |     |  0  |  0  |``\n",
    "\n",
    "    would be 0.3 + .9 = 1.2.  This task was proposed in [1]_ and explored in\n",
    "    e.g. [2]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    min_length : int\n",
    "        Minimum sequence length.\n",
    "\n",
    "    max_length : int\n",
    "        Maximum sequence length.\n",
    "\n",
    "    n_batch : int\n",
    "        Number of samples in the batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    X : np.ndarray\n",
    "        Input to the network, of shape (n_batch, max_length, 2), where the last\n",
    "        dimension corresponds to the two sequences shown above.\n",
    "\n",
    "    y : np.ndarray\n",
    "        Correct output for each sample, shape (n_batch,).\n",
    "\n",
    "    mask : np.ndarray\n",
    "        A binary matrix of shape (n_batch, max_length) where ``mask[i, j] = 1``\n",
    "        when ``j <= (length of sequence i)`` and ``mask[i, j] = 0`` when ``j >\n",
    "        (length of sequence i)``.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] Hochreiter, Sepp, and JÃ¼rgen Schmidhuber. \"Long short-term memory.\"\n",
    "    Neural computation 9.8 (1997): 1735-1780.\n",
    "\n",
    "    .. [2] Sutskever, Ilya, et al. \"On the importance of initialization and\n",
    "    momentum in deep learning.\" Proceedings of the 30th international\n",
    "    conference on machine learning (ICML-13). 2013.\n",
    "\n",
    "    '''\n",
    "    # Generate X - we'll fill the last dimension later\n",
    "    X = np.concatenate([np.random.uniform(size=(n_batch, max_length, 1)),\n",
    "                        np.zeros((n_batch, max_length, 1))],\n",
    "                       axis=-1)\n",
    "    mask = np.zeros((n_batch, max_length))\n",
    "    y = np.zeros((n_batch,))\n",
    "    # Compute masks and correct values\n",
    "    for n in range(n_batch):\n",
    "        # Randomly choose the sequence length\n",
    "        length = np.random.randint(min_length, max_length)\n",
    "        # Make the mask for this sample 1 within the range of length\n",
    "        mask[n, :length] = 1\n",
    "        # Zero out X after the end of the sequence\n",
    "        X[n, length:, 0] = 0\n",
    "        # Set the second dimension to 1 at the indices to add\n",
    "        X[n, np.random.randint(length/10), 1] = 1\n",
    "        X[n, np.random.randint(length/2, length), 1] = 1\n",
    "        # Multiply and sum the dimensions of X to get the target value\n",
    "        y[n] = np.sum(X[n, :, 0]*X[n, :, 1])\n",
    "    # Center the inputs and outputs\n",
    "    X -= X.reshape(-1, 2).mean(axis=0)\n",
    "    y -= y.mean()\n",
    "    return (\n",
    "        X.astype(np.float32), \n",
    "        y.astype(np.float32),\n",
    "        mask.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, _ = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 55, 2), (1000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic recurrent layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the vanilla mink recurrent layer. You can pass it any `tensorflow.nn.rnn_cell`. By default, it uses `tensorflow.nn.rnn_cell.BasicRNNCell` with 100 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0 = InputLayer()\n",
    "l1 = RecurrentLayer(l0)\n",
    "l2 = DenseLayer(l1, nonlinearity=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    l2,\n",
    "    update=UPDATE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    verbose=1,\n",
    "    session_kwargs=SESSION_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 5501 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "|   # | name      | size   |\n",
      "|----:|:----------|:-------|\n",
      "|   0 | input     | 55x2   |\n",
      "|   1 | recurrent | 55x100 |\n",
      "|   2 | dense     | 1      |\n",
      "\n",
      "|   epoch |   train loss |     dur |\n",
      "|--------:|-------------:|--------:|\n",
      "|       1 |      \u001b[36m0.18565\u001b[0m | 0.16853 |\n",
      "|       2 |      \u001b[36m0.17970\u001b[0m | 0.09502 |\n",
      "|       3 |      \u001b[36m0.17496\u001b[0m | 0.09938 |\n",
      "|       4 |      \u001b[36m0.17092\u001b[0m | 0.09245 |\n",
      "|       5 |      \u001b[36m0.16716\u001b[0m | 0.08941 |\n",
      "|       6 |      \u001b[36m0.16376\u001b[0m | 0.09735 |\n",
      "|       7 |      \u001b[36m0.16062\u001b[0m | 0.08589 |\n",
      "|       8 |      \u001b[36m0.16005\u001b[0m | 0.10094 |\n",
      "|       9 |      0.16674 | 0.09322 |\n",
      "|      10 |      0.16366 | 0.09473 |\n",
      "|      11 |      \u001b[36m0.15644\u001b[0m | 0.09227 |\n",
      "|      12 |      0.15760 | 0.09193 |\n",
      "|      13 |      0.15819 | 0.09398 |\n",
      "|      14 |      0.15886 | 0.08454 |\n",
      "|      15 |      0.15832 | 0.08232 |\n",
      "|      16 |      0.15726 | 0.08955 |\n",
      "|      17 |      \u001b[36m0.15576\u001b[0m | 0.09331 |\n",
      "|      18 |      \u001b[36m0.15558\u001b[0m | 0.09303 |\n",
      "|      19 |      \u001b[36m0.15454\u001b[0m | 0.09387 |\n",
      "|      20 |      \u001b[36m0.15426\u001b[0m | 0.09373 |\n",
      "|      21 |      0.15504 | 0.09529 |\n",
      "|      22 |      \u001b[36m0.14967\u001b[0m | 0.09102 |\n",
      "|      23 |      0.15024 | 0.09803 |\n",
      "|      24 |      0.15067 | 0.09157 |\n",
      "|      25 |      0.15078 | 0.08924 |\n",
      "|      26 |      0.15085 | 0.09622 |\n",
      "|      27 |      0.15050 | 0.08654 |\n",
      "|      28 |      0.15087 | 0.08506 |\n",
      "|      29 |      0.15012 | 0.08668 |\n",
      "|      30 |      0.14996 | 0.08626 |\n",
      "|      31 |      \u001b[36m0.14922\u001b[0m | 0.09001 |\n",
      "|      32 |      \u001b[36m0.14921\u001b[0m | 0.09712 |\n",
      "|      33 |      \u001b[36m0.14919\u001b[0m | 0.09206 |\n",
      "|      34 |      \u001b[36m0.14896\u001b[0m | 0.08435 |\n",
      "|      35 |      \u001b[36m0.14874\u001b[0m | 0.08765 |\n",
      "|      36 |      \u001b[36m0.14824\u001b[0m | 0.08519 |\n",
      "|      37 |      \u001b[36m0.14799\u001b[0m | 0.08433 |\n",
      "|      38 |      \u001b[36m0.14791\u001b[0m | 0.08334 |\n",
      "|      39 |      \u001b[36m0.14767\u001b[0m | 0.08370 |\n",
      "|      40 |      \u001b[36m0.14761\u001b[0m | 0.08411 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetRegressor(batch_iterator_test=128, batch_iterator_train=128,\n",
       "          encoder=None,\n",
       "          layer=DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
       "      incoming=RecurrentLayer(cell=None,\n",
       "        incoming=InputLayer(Xs=None, make_logs=False, name=None, ys=None),\n",
       "        make_logs=False, name=None, sequence_length=None),\n",
       "      make_logs=False, name=None, nonlinearity=Tanh(), num_units=1),\n",
       "          max_epochs=40, objective=MeanSquaredError(),\n",
       "          on_epoch_finished=(PrintTrainProgress(first_iteration=False, floatfmt='.5f', tablefmt='pipe'),),\n",
       "          on_training_started=(PrintLayerInfo(tablefmt='pipe'),),\n",
       "          session_kwargs={'config': gpu_options {\n",
       "  per_process_gpu_memory_fraction: 0.5\n",
       "}\n",
       "},\n",
       "          update=RMSProp(decay=0.9, learning_rate=0.0015, momentum=0.0),\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing an `LSTMCell` to the basic `RecurrentLayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how you can pass an `LSTMCell` to the `RecurrentLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units=100,\n",
    "    use_peepholes=True,\n",
    "    state_is_tuple=True,\n",
    "    cell_clip=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0 = InputLayer()\n",
    "l1 = RecurrentLayer(l0, cell=cell)\n",
    "l2 = DenseLayer(l1, nonlinearity=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    l2,\n",
    "    update=UPDATE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    verbose=1,\n",
    "    session_kwargs=SESSION_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 5501 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "|   # | name      | size   |\n",
      "|----:|:----------|:-------|\n",
      "|   0 | input     | 55x2   |\n",
      "|   1 | recurrent | 55x100 |\n",
      "|   2 | dense     | 1      |\n",
      "\n",
      "|   epoch |   train loss |     dur |\n",
      "|--------:|-------------:|--------:|\n",
      "|       1 |      \u001b[36m0.17640\u001b[0m | 0.37055 |\n",
      "|       2 |      \u001b[36m0.17581\u001b[0m | 0.32639 |\n",
      "|       3 |      \u001b[36m0.17504\u001b[0m | 0.32380 |\n",
      "|       4 |      \u001b[36m0.17403\u001b[0m | 0.32425 |\n",
      "|       5 |      \u001b[36m0.17270\u001b[0m | 0.31516 |\n",
      "|       6 |      \u001b[36m0.17097\u001b[0m | 0.32826 |\n",
      "|       7 |      \u001b[36m0.16875\u001b[0m | 0.32109 |\n",
      "|       8 |      \u001b[36m0.16600\u001b[0m | 0.31773 |\n",
      "|       9 |      \u001b[36m0.16309\u001b[0m | 0.31548 |\n",
      "|      10 |      \u001b[36m0.16021\u001b[0m | 0.31493 |\n",
      "|      11 |      \u001b[36m0.15758\u001b[0m | 0.31299 |\n",
      "|      12 |      \u001b[36m0.15543\u001b[0m | 0.31409 |\n",
      "|      13 |      \u001b[36m0.15362\u001b[0m | 0.31559 |\n",
      "|      14 |      \u001b[36m0.15265\u001b[0m | 0.34484 |\n",
      "|      15 |      \u001b[36m0.15199\u001b[0m | 0.31772 |\n",
      "|      16 |      0.15214 | 0.31784 |\n",
      "|      17 |      \u001b[36m0.14959\u001b[0m | 0.31354 |\n",
      "|      18 |      0.14987 | 0.31277 |\n",
      "|      19 |      \u001b[36m0.14906\u001b[0m | 0.31769 |\n",
      "|      20 |      \u001b[36m0.14711\u001b[0m | 0.32223 |\n",
      "|      21 |      \u001b[36m0.14580\u001b[0m | 0.32307 |\n",
      "|      22 |      \u001b[36m0.14501\u001b[0m | 0.31976 |\n",
      "|      23 |      \u001b[36m0.14287\u001b[0m | 0.33361 |\n",
      "|      24 |      \u001b[36m0.14142\u001b[0m | 0.34238 |\n",
      "|      25 |      \u001b[36m0.14013\u001b[0m | 0.32929 |\n",
      "|      26 |      \u001b[36m0.13498\u001b[0m | 0.33375 |\n",
      "|      27 |      \u001b[36m0.13224\u001b[0m | 0.32375 |\n",
      "|      28 |      0.13802 | 0.32389 |\n",
      "|      29 |      \u001b[36m0.11688\u001b[0m | 0.31934 |\n",
      "|      30 |      0.11728 | 0.31828 |\n",
      "|      31 |      \u001b[36m0.10851\u001b[0m | 0.32101 |\n",
      "|      32 |      \u001b[36m0.10192\u001b[0m | 0.32630 |\n",
      "|      33 |      \u001b[36m0.09075\u001b[0m | 0.31871 |\n",
      "|      34 |      \u001b[36m0.08680\u001b[0m | 0.32724 |\n",
      "|      35 |      0.09190 | 0.32104 |\n",
      "|      36 |      \u001b[36m0.08266\u001b[0m | 0.31391 |\n",
      "|      37 |      \u001b[36m0.07132\u001b[0m | 0.31721 |\n",
      "|      38 |      0.08734 | 0.31512 |\n",
      "|      39 |      \u001b[36m0.06297\u001b[0m | 0.31353 |\n",
      "|      40 |      0.06797 | 0.32721 |\n",
      "|      41 |      \u001b[36m0.06122\u001b[0m | 0.32660 |\n",
      "|      42 |      \u001b[36m0.05784\u001b[0m | 0.32397 |\n",
      "|      43 |      0.06523 | 0.32506 |\n",
      "|      44 |      \u001b[36m0.04703\u001b[0m | 0.32651 |\n",
      "|      45 |      0.05824 | 0.32222 |\n",
      "|      46 |      \u001b[36m0.03308\u001b[0m | 0.34851 |\n",
      "|      47 |      0.06386 | 0.33188 |\n",
      "|      48 |      0.05439 | 0.31750 |\n",
      "|      49 |      0.03701 | 0.31488 |\n",
      "|      50 |      0.03809 | 0.31947 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetRegressor(batch_iterator_test=128, batch_iterator_train=128,\n",
       "          encoder=None,\n",
       "          layer=DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
       "      incoming=RecurrentLayer(cell=<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7f4b8af735c0>,\n",
       "        incoming=InputLayer(Xs=None, make_logs=False, name=None, ys=None),\n",
       "        make_logs=False, name=None, sequence_length=None),\n",
       "      make_logs=False, name=None, nonlinearity=Tanh(), num_units=1),\n",
       "          max_epochs=40, objective=MeanSquaredError(),\n",
       "          on_epoch_finished=(PrintTrainProgress(first_iteration=False, floatfmt='.5f', tablefmt='pipe'),),\n",
       "          on_training_started=(PrintLayerInfo(tablefmt='pipe'),),\n",
       "          session_kwargs={'config': gpu_options {\n",
       "  per_process_gpu_memory_fraction: 0.5\n",
       "}\n",
       "},\n",
       "          update=RMSProp(decay=0.9, learning_rate=0.0015, momentum=0.0),\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing an `LSTMCell` to the `RecurrentLayer`, it is possible to directly use mink's LSTMLayer. Under the hood, both approaches amount to the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0 = InputLayer()\n",
    "l1 = LSTMLayer(l0, use_peepholes=True, cell_clip=100)\n",
    "l2 = DenseLayer(l1, nonlinearity=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    l2,\n",
    "    update=UPDATE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    verbose=1,\n",
    "    session_kwargs=SESSION_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 5501 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "|   # | name   | size   |\n",
      "|----:|:-------|:-------|\n",
      "|   0 | input  | 55x2   |\n",
      "|   1 | lstm   | 55x100 |\n",
      "|   2 | dense  | 1      |\n",
      "\n",
      "|   epoch |   train loss |     dur |\n",
      "|--------:|-------------:|--------:|\n",
      "|       1 |      \u001b[36m0.17374\u001b[0m | 0.33540 |\n",
      "|       2 |      \u001b[36m0.17351\u001b[0m | 0.31706 |\n",
      "|       3 |      \u001b[36m0.17317\u001b[0m | 0.31294 |\n",
      "|       4 |      \u001b[36m0.17267\u001b[0m | 0.31776 |\n",
      "|       5 |      \u001b[36m0.17197\u001b[0m | 0.31788 |\n",
      "|       6 |      \u001b[36m0.17099\u001b[0m | 0.31407 |\n",
      "|       7 |      \u001b[36m0.16974\u001b[0m | 0.31321 |\n",
      "|       8 |      \u001b[36m0.16830\u001b[0m | 0.31218 |\n",
      "|       9 |      \u001b[36m0.16644\u001b[0m | 0.32088 |\n",
      "|      10 |      \u001b[36m0.16373\u001b[0m | 0.31720 |\n",
      "|      11 |      \u001b[36m0.16044\u001b[0m | 0.31294 |\n",
      "|      12 |      \u001b[36m0.15711\u001b[0m | 0.31125 |\n",
      "|      13 |      \u001b[36m0.15631\u001b[0m | 0.31411 |\n",
      "|      14 |      \u001b[36m0.15391\u001b[0m | 0.31486 |\n",
      "|      15 |      \u001b[36m0.15093\u001b[0m | 0.30924 |\n",
      "|      16 |      0.15225 | 0.31204 |\n",
      "|      17 |      0.15176 | 0.31149 |\n",
      "|      18 |      \u001b[36m0.14772\u001b[0m | 0.31459 |\n",
      "|      19 |      \u001b[36m0.14702\u001b[0m | 0.31388 |\n",
      "|      20 |      0.14783 | 0.30981 |\n",
      "|      21 |      \u001b[36m0.14664\u001b[0m | 0.31009 |\n",
      "|      22 |      \u001b[36m0.14423\u001b[0m | 0.30853 |\n",
      "|      23 |      \u001b[36m0.14352\u001b[0m | 0.31437 |\n",
      "|      24 |      \u001b[36m0.14142\u001b[0m | 0.31583 |\n",
      "|      25 |      \u001b[36m0.14030\u001b[0m | 0.31578 |\n",
      "|      26 |      \u001b[36m0.13717\u001b[0m | 0.31834 |\n",
      "|      27 |      \u001b[36m0.12973\u001b[0m | 0.31483 |\n",
      "|      28 |      0.13638 | 0.31509 |\n",
      "|      29 |      \u001b[36m0.12398\u001b[0m | 0.31621 |\n",
      "|      30 |      \u001b[36m0.11011\u001b[0m | 0.31374 |\n",
      "|      31 |      0.11622 | 0.33750 |\n",
      "|      32 |      0.12074 | 0.34899 |\n",
      "|      33 |      \u001b[36m0.09886\u001b[0m | 0.31600 |\n",
      "|      34 |      \u001b[36m0.09694\u001b[0m | 0.32805 |\n",
      "|      35 |      \u001b[36m0.09670\u001b[0m | 0.32006 |\n",
      "|      36 |      \u001b[36m0.08817\u001b[0m | 0.33003 |\n",
      "|      37 |      \u001b[36m0.07703\u001b[0m | 0.34921 |\n",
      "|      38 |      0.08017 | 0.33456 |\n",
      "|      39 |      \u001b[36m0.07673\u001b[0m | 0.33427 |\n",
      "|      40 |      \u001b[36m0.06216\u001b[0m | 0.33629 |\n",
      "|      41 |      0.07418 | 0.34502 |\n",
      "|      42 |      0.06830 | 0.31579 |\n",
      "|      43 |      0.07183 | 0.32306 |\n",
      "|      44 |      \u001b[36m0.05189\u001b[0m | 0.31774 |\n",
      "|      45 |      0.06269 | 0.31382 |\n",
      "|      46 |      \u001b[36m0.03728\u001b[0m | 0.31322 |\n",
      "|      47 |      0.07034 | 0.31328 |\n",
      "|      48 |      0.03980 | 0.31535 |\n",
      "|      49 |      0.05104 | 0.31559 |\n",
      "|      50 |      \u001b[36m0.03695\u001b[0m | 0.31417 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetRegressor(batch_iterator_test=128, batch_iterator_train=128,\n",
       "          encoder=None,\n",
       "          layer=DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
       "      incoming=LSTMLayer(cell_clip=100,\n",
       "     incoming=InputLayer(Xs=None, make_logs=False, name=None, ys=None),\n",
       "     make_logs=False, name=None, nonlinearity=Tanh(), num_units=100,\n",
       "     sequence_length=None, use_peepholes=True),\n",
       "      make_logs=False, name=None, nonlinearity=Tanh(), num_units=1),\n",
       "          max_epochs=40, objective=MeanSquaredError(),\n",
       "          on_epoch_finished=(PrintTrainProgress(first_iteration=False, floatfmt='.5f', tablefmt='pipe'),),\n",
       "          on_training_started=(PrintLayerInfo(tablefmt='pipe'),),\n",
       "          session_kwargs={'config': gpu_options {\n",
       "  per_process_gpu_memory_fraction: 0.5\n",
       "}\n",
       "},\n",
       "          update=RMSProp(decay=0.9, learning_rate=0.0015, momentum=0.0),\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, mink also currently supports a layer using the Gated Recurrent Unit (GRU) cell from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0 = InputLayer()\n",
    "l1 = GRULayer(l0)\n",
    "l2 = DenseLayer(l1, nonlinearity=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    l2,\n",
    "    update=UPDATE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    verbose=1,\n",
    "    session_kwargs=SESSION_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 5501 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "|   # | name   | size   |\n",
      "|----:|:-------|:-------|\n",
      "|   0 | input  | 55x2   |\n",
      "|   1 | gru    | 55x100 |\n",
      "|   2 | dense  | 1      |\n",
      "\n",
      "|   epoch |   train loss |     dur |\n",
      "|--------:|-------------:|--------:|\n",
      "|       1 |      \u001b[36m0.17357\u001b[0m | 0.25589 |\n",
      "|       2 |      \u001b[36m0.17340\u001b[0m | 0.24200 |\n",
      "|       3 |      \u001b[36m0.17315\u001b[0m | 0.24157 |\n",
      "|       4 |      \u001b[36m0.17277\u001b[0m | 0.24604 |\n",
      "|       5 |      \u001b[36m0.17222\u001b[0m | 0.23385 |\n",
      "|       6 |      \u001b[36m0.17141\u001b[0m | 0.24360 |\n",
      "|       7 |      \u001b[36m0.17025\u001b[0m | 0.24318 |\n",
      "|       8 |      \u001b[36m0.16864\u001b[0m | 0.23332 |\n",
      "|       9 |      \u001b[36m0.16648\u001b[0m | 0.22514 |\n",
      "|      10 |      \u001b[36m0.16376\u001b[0m | 0.22569 |\n",
      "|      11 |      \u001b[36m0.16053\u001b[0m | 0.23903 |\n",
      "|      12 |      \u001b[36m0.15703\u001b[0m | 0.24755 |\n",
      "|      13 |      \u001b[36m0.15374\u001b[0m | 0.24099 |\n",
      "|      14 |      \u001b[36m0.15126\u001b[0m | 0.24317 |\n",
      "|      15 |      \u001b[36m0.15013\u001b[0m | 0.23944 |\n",
      "|      16 |      \u001b[36m0.14979\u001b[0m | 0.24261 |\n",
      "|      17 |      \u001b[36m0.14804\u001b[0m | 0.24526 |\n",
      "|      18 |      \u001b[36m0.14707\u001b[0m | 0.24895 |\n",
      "|      19 |      \u001b[36m0.14666\u001b[0m | 0.25129 |\n",
      "|      20 |      \u001b[36m0.14593\u001b[0m | 0.25236 |\n",
      "|      21 |      \u001b[36m0.14321\u001b[0m | 0.22802 |\n",
      "|      22 |      \u001b[36m0.14212\u001b[0m | 0.22572 |\n",
      "|      23 |      \u001b[36m0.14023\u001b[0m | 0.22441 |\n",
      "|      24 |      \u001b[36m0.13771\u001b[0m | 0.25268 |\n",
      "|      25 |      \u001b[36m0.13369\u001b[0m | 0.23318 |\n",
      "|      26 |      \u001b[36m0.13346\u001b[0m | 0.22422 |\n",
      "|      27 |      \u001b[36m0.11837\u001b[0m | 0.22565 |\n",
      "|      28 |      \u001b[36m0.11349\u001b[0m | 0.22666 |\n",
      "|      29 |      \u001b[36m0.09525\u001b[0m | 0.22780 |\n",
      "|      30 |      \u001b[36m0.08825\u001b[0m | 0.22678 |\n",
      "|      31 |      0.09965 | 0.24787 |\n",
      "|      32 |      0.10696 | 0.23397 |\n",
      "|      33 |      \u001b[36m0.06415\u001b[0m | 0.22601 |\n",
      "|      34 |      0.06953 | 0.22387 |\n",
      "|      35 |      \u001b[36m0.05403\u001b[0m | 0.22736 |\n",
      "|      36 |      0.06257 | 0.22668 |\n",
      "|      37 |      \u001b[36m0.04614\u001b[0m | 0.23639 |\n",
      "|      38 |      0.06544 | 0.24395 |\n",
      "|      39 |      \u001b[36m0.03577\u001b[0m | 0.22653 |\n",
      "|      40 |      0.05295 | 0.22905 |\n",
      "|      41 |      0.04257 | 0.22810 |\n",
      "|      42 |      0.04460 | 0.22870 |\n",
      "|      43 |      0.03882 | 0.22510 |\n",
      "|      44 |      0.03809 | 0.24007 |\n",
      "|      45 |      0.04174 | 0.24245 |\n",
      "|      46 |      \u001b[36m0.03131\u001b[0m | 0.24014 |\n",
      "|      47 |      0.03807 | 0.23566 |\n",
      "|      48 |      0.03156 | 0.22563 |\n",
      "|      49 |      \u001b[36m0.03036\u001b[0m | 0.22507 |\n",
      "|      50 |      \u001b[36m0.02409\u001b[0m | 0.22780 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetRegressor(batch_iterator_test=128, batch_iterator_train=128,\n",
       "          encoder=None,\n",
       "          layer=DenseLayer(W=GlorotUniform(c01b=False, gain=1.0), b=Constant(value=0.0),\n",
       "      incoming=GRULayer(incoming=InputLayer(Xs=None, make_logs=False, name=None, ys=None),\n",
       "     make_logs=False, name=None, nonlinearity=Tanh(), num_units=100,\n",
       "     sequence_length=None),\n",
       "      make_logs=False, name=None, nonlinearity=Tanh(), num_units=1),\n",
       "          max_epochs=40, objective=MeanSquaredError(),\n",
       "          on_epoch_finished=(PrintTrainProgress(first_iteration=False, floatfmt='.5f', tablefmt='pipe'),),\n",
       "          on_training_started=(PrintLayerInfo(tablefmt='pipe'),),\n",
       "          session_kwargs={'config': gpu_options {\n",
       "  per_process_gpu_memory_fraction: 0.5\n",
       "}\n",
       "},\n",
       "          update=RMSProp(decay=0.9, learning_rate=0.0015, momentum=0.0),\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X, y, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
